{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I chose to use transformers, through project BERT, in order to take the context of the embeddings into account.\n",
    "The embeddings of the contexts (paragraphs) will be generated once and stored in a file. This same file will be used as an input in the algorithm and read to compare the contexts with the embedding of the question.\n",
    "\n",
    "NB : I will not be using the whole training set (in order not to go beyond the capacities of my computer while generating the embeddings). Also, I did not train the BERT model on the train data (it would require too much time and the access to GPU ressources)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zipfile import ZipFile \n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from annoy import AnnoyIndex\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import os\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autotime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2 ms\n"
     ]
    }
   ],
   "source": [
    "def json_to_dataframe(input_file_path, record_path = ['data','paragraphs','qas','answers'],\n",
    "                           verbose = 1):\n",
    "    \"\"\"\n",
    "    input_file_path: path to the squad json file.\n",
    "    record_path: path to deepest level in json file default value is\n",
    "    ['data','paragraphs','qas','answers']\n",
    "    verbose: 0 to suppress it default is 1\n",
    "    \"\"\"\n",
    "    if verbose:\n",
    "        print(\"Reading the json file\")    \n",
    "    with ZipFile(input_file_path, \"r\") as z:\n",
    "        for filename in z.namelist():  \n",
    "            print(filename)  \n",
    "            with z.open(filename) as f:  \n",
    "                data = f.read()  \n",
    "                file = json.loads(data.decode(\"utf-8\"))\n",
    "    if verbose:\n",
    "        print(\"processing...\")\n",
    "    # parsing different level's in the json file\n",
    "    js = pd.io.json.json_normalize(file , record_path )\n",
    "    m = pd.io.json.json_normalize(file, record_path[:-1] )\n",
    "    r = pd.io.json.json_normalize(file,record_path[:-2])\n",
    "\n",
    "    #combining it into single dataframe\n",
    "    idx = np.repeat(r['context'].values, r.qas.str.len())\n",
    "    #     ndx  = np.repeat(m['id'].values,m['answers'].str.len())\n",
    "    m['context'] = idx\n",
    "    #     js['q_idx'] = ndx\n",
    "    main = m[['id','question','context','answers']].set_index('id').reset_index()\n",
    "    main['c_id'] = main['context'].factorize()[0]\n",
    "    if verbose:\n",
    "        print(\"shape of the dataframe is {}\".format(main.shape))\n",
    "        print(\"Done\")\n",
    "    return main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading the json file\n",
      "train.json\n",
      "processing...\n",
      "shape of the dataframe is (20731, 5)\n",
      "Done\n",
      "time: 943 ms\n"
     ]
    }
   ],
   "source": [
    "input_file_path = 'train.json.zip'\n",
    "record_path = ['data','paragraphs','qas','answers']\n",
    "verbose = 0\n",
    "train_data = json_to_dataframe(input_file_path, record_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20731"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 6 ms\n"
     ]
    }
   ],
   "source": [
    "len(train_data[\"context\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4920"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 29 ms\n"
     ]
    }
   ],
   "source": [
    "len(train_data[\"context\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4920"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 37.8 ms\n"
     ]
    }
   ],
   "source": [
    "bis = train_data.drop_duplicates(subset=[\"context\"])\n",
    "len(bis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining a function to store the embeddings in order to boost the process "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1 ms\n"
     ]
    }
   ],
   "source": [
    "def store_data(data, name_output = \"encoding.ann\"):\n",
    "\n",
    "    items = data\n",
    "    \n",
    "    model = SentenceTransformer('distiluse-base-multilingual-cased')\n",
    "\n",
    "    f = 512\n",
    "    t = AnnoyIndex(f, 'angular')  # Length of item vector that will be indexed\n",
    "    \n",
    "    items = items.set_index(\"c_id\")\n",
    "    \n",
    "    for i, row in items.iterrows():\n",
    "        v = model.encode(row[\"context\"])\n",
    "        t.add_item(i, v)\n",
    "\n",
    "    t.build(10) # 10 trees\n",
    "    t.save(name_output)\n",
    "\n",
    "    print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "time: 9.86 s\n"
     ]
    }
   ],
   "source": [
    "store_data(bis[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This operation is only done once in a while. When new contexts are added, the file must be updated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the embeddings and encoding the query to compare it to them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The metric used to compute the similarity between the query and each context is the cosine similarity (the parameter \"angular\" was used as a metric in annoy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 963 Âµs\n"
     ]
    }
   ],
   "source": [
    "def load_corpus(file_name = 'encoding.ann'):\n",
    "    f = 512\n",
    "    u = AnnoyIndex(f, 'angular')\n",
    "    u.load(file_name)  # super fast, will just mmap the file\n",
    "    return u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.01 ms\n"
     ]
    }
   ],
   "source": [
    "def import_enc(file_name = \"encoding.ann\"):\n",
    "    model = SentenceTransformer('distiluse-base-multilingual-cased')\n",
    "    embeddings = load_corpus(file_name)\n",
    "    return(model, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in callback <function LineWatcher.stop at 0x0000017B8B5A31F8> (for post_run_cell):\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\backcall\\backcall.py\u001b[0m in \u001b[0;36madapted\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    102\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[1;31m#            print(args, kwargs, unmatched_pos, cut_positional, unmatched_kw)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 104\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    105\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0madapted\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\autotime.py\u001b[0m in \u001b[0;36mstop\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart_time\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m             \u001b[0mdiff\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m             \u001b[1;32massert\u001b[0m \u001b[0mdiff\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'time: %s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mformat_delta\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdiff\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def produce_prediction(query_text, model, embeddings, top_n = 3):\n",
    "    query = model.encode(query_text)\n",
    "    nearest = embeddings.get_nns_by_vector(query, top_n, include_distances = True)\n",
    "    return nearest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.9 s\n"
     ]
    }
   ],
   "source": [
    "model, embeddings = import_enc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.04 ms\n"
     ]
    }
   ],
   "source": [
    "query_text = \"Pour quelle raison on ne put plus observer CÃ©rÃ¨s ?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([17, 2, 18], [1.2273740768432617, 1.2467377185821533, 1.2482205629348755])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 37.7 ms\n"
     ]
    }
   ],
   "source": [
    "nearest = produce_prediction(query_text, model, embeddings)\n",
    "nearest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lorsque CÃ©rÃ¨s est en opposition Ã  proximitÃ© de son pÃ©rihÃ©lie, il peut atteindre une magnitude apparente de +6,7. On considÃ¨re gÃ©nÃ©ralement que cette valeur est trop faible pour que l'objet soit visible Ã  l'Åil nu, mais il est nÃ©anmoins possible pour une personne dotÃ©e d'une excellente vue et dans des conditions d'observation exceptionnelles de percevoir la planÃ¨te naine. Les seuls astÃ©roÃ¯des pouvant atteindre une telle magnitude sont Vesta et, pendant de rares oppositions Ã  leur pÃ©rihÃ©lie, Pallas et Iris. Au maximum de sa luminositÃ©, CÃ©rÃ¨s n'est pas l'astÃ©roÃ¯de le plus brillant ; Vesta peut atteindre la magnitude +5,4, la derniÃ¨re fois en mai et juin 2007. Aux conjonctions, CÃ©rÃ¨s atteint la magnitude de +9,3, ce qui correspond aux objets les moins lumineux qui puissent Ãªtre visibles Ã  l'aide de jumelles 10Ã50. La planÃ¨te naine peut donc Ãªtre vue aux jumelles dÃ¨s qu'elle est au-dessus de l'horizon par une nuit noire. Pallas et Iris sont invisibles aux jumelles par de petites Ã©longations.\n",
      "\n",
      "Peu aprÃ¨s sa dÃ©couverte, CÃ©rÃ¨s s'approcha trop prÃ¨s du Soleil et ne put Ãªtre observÃ©e Ã  nouveau ; les autres astronomes ne purent confirmer les observations de Piazzi avant la fin de l'annÃ©e. Cependant, aprÃ¨s une telle durÃ©e, il Ã©tait difficile de prÃ©dire la position exacte de CÃ©rÃ¨s. Afin de retrouver l'astÃ©roÃ¯de, Carl Friedrich Gauss dÃ©veloppa une mÃ©thode de dÃ©duction de l'orbite basÃ©e sur trois observations. En l'espace de quelques semaines, il prÃ©dit celle de CÃ©rÃ¨s et communiqua ses rÃ©sultats Ã  Franz Xaver von Zach, Ã©diteur du Monatliche Correspondenz. Le 31 dÃ©cembre 1801, von Zach et Heinrich Olbers confirmÃ¨rent que CÃ©rÃ¨s avait Ã©tÃ© retrouvÃ©e prÃ¨s de la position prÃ©vue, validant ainsi la mÃ©thode.\n",
      "\n",
      "En mars 2018, de nouvelles conclusions sont publiÃ©es sur la base de donnÃ©es observÃ©es par Dawn d'avril Ã  octobre 2016. Elles mettent en Ã©vidence une structure plus complexe que prÃ©vu, et surtout elle montre qu'il s'agit d'une planÃ¨te naine encore trÃ¨s active avec une augmentation de la quantitÃ© d'eau glacÃ©e sur les murs de cratÃ¨re,. Il s'agit lÃ  de la premiÃ¨re fois oÃ¹ une Ã©volution de la surface de CÃ©rÃ¨s est mise en Ã©vidence. Ces observations permettent Ã©galement au grand public de dÃ©couvrir des images extraordinaires de CÃ©rÃ¨s.\n",
      "time: 7 ms\n"
     ]
    }
   ],
   "source": [
    "print(*reduce(pd.DataFrame.append, map(lambda i: bis[bis.c_id == i], nearest[0]))[\"context\"].tolist(), sep = '\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I can see that the contexts are \"close\" to the query (in terms of main subject). The problem here is that, just by comparing the closest context to the real context linked to that query, I can see that I did not get the right context. The similiarities between the right one (1.2467377185821533) and the one predicted (1.2273740768432617) are quite close.\n",
    "I can also underline that the predicted context contains the word \"observation\", \"invisibles\", \"percevoir\", thus leading the algorithm to pick him as the closest context (the query contains the word \"observer\")."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate the model, I will use the F1 score. The score would surely be greater with a training of the BERT model on our data.\n",
    "For each context, I am computing the terms required for the calculation of the F1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading the json file\n",
      "valid.json\n",
      "processing...\n",
      "shape of the dataframe is (3188, 5)\n",
      "Done\n",
      "time: 141 ms\n"
     ]
    }
   ],
   "source": [
    "input_file_path_valid = 'valid.json.zip'\n",
    "record_path_valid = ['data','paragraphs','qas','answers']\n",
    "verbose = 0\n",
    "valid_data = json_to_dataframe(input_file_path_valid, record_path_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>question</th>\n",
       "      <th>context</th>\n",
       "      <th>answers</th>\n",
       "      <th>c_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>a524504f-0816-4f58-9f2d-27f82a85c73d</td>\n",
       "      <td>Que concerne principalement les documents ?</td>\n",
       "      <td>Les deux tableaux sont certes dÃ©crits par des ...</td>\n",
       "      <td>[{'answer_start': 161, 'text': 'La Vierge aux ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>8a72ad1c-b2fe-4fe6-9f87-35fcb713cf38</td>\n",
       "      <td>Par quoi sont dÃ©crit les deux tableaux ?</td>\n",
       "      <td>Les deux tableaux sont certes dÃ©crits par des ...</td>\n",
       "      <td>[{'answer_start': 46, 'text': 'documents conte...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>b2db7f77-f6a7-4c3d-9274-9807f9764e97</td>\n",
       "      <td>Quels types d'objets sont les deux tableaux au...</td>\n",
       "      <td>Les deux tableaux sont certes dÃ©crits par des ...</td>\n",
       "      <td>[{'answer_start': 204, 'text': 'objets de spÃ©c...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>31ec5079-7d62-43b9-8a5c-49f8b7ad6f5a</td>\n",
       "      <td>Sur quelle jambe les personnages se tiennent-t...</td>\n",
       "      <td>Les deux panneaux prÃ©sentent de nombreuses sim...</td>\n",
       "      <td>[{'answer_start': 242, 'text': 'droite'}]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>008a2ca6-a589-4751-ba59-f7f762874785</td>\n",
       "      <td>Quel pied avancent les personnages ?</td>\n",
       "      <td>Les deux panneaux prÃ©sentent de nombreuses sim...</td>\n",
       "      <td>[{'answer_start': 271, 'text': 'gauche'}]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     id  \\\n",
       "0  a524504f-0816-4f58-9f2d-27f82a85c73d   \n",
       "1  8a72ad1c-b2fe-4fe6-9f87-35fcb713cf38   \n",
       "2  b2db7f77-f6a7-4c3d-9274-9807f9764e97   \n",
       "3  31ec5079-7d62-43b9-8a5c-49f8b7ad6f5a   \n",
       "4  008a2ca6-a589-4751-ba59-f7f762874785   \n",
       "\n",
       "                                            question  \\\n",
       "0        Que concerne principalement les documents ?   \n",
       "1           Par quoi sont dÃ©crit les deux tableaux ?   \n",
       "2  Quels types d'objets sont les deux tableaux au...   \n",
       "3  Sur quelle jambe les personnages se tiennent-t...   \n",
       "4              Quel pied avancent les personnages ?    \n",
       "\n",
       "                                             context  \\\n",
       "0  Les deux tableaux sont certes dÃ©crits par des ...   \n",
       "1  Les deux tableaux sont certes dÃ©crits par des ...   \n",
       "2  Les deux tableaux sont certes dÃ©crits par des ...   \n",
       "3  Les deux panneaux prÃ©sentent de nombreuses sim...   \n",
       "4  Les deux panneaux prÃ©sentent de nombreuses sim...   \n",
       "\n",
       "                                             answers  c_id  \n",
       "0  [{'answer_start': 161, 'text': 'La Vierge aux ...     0  \n",
       "1  [{'answer_start': 46, 'text': 'documents conte...     0  \n",
       "2  [{'answer_start': 204, 'text': 'objets de spÃ©c...     0  \n",
       "3          [{'answer_start': 242, 'text': 'droite'}]     1  \n",
       "4          [{'answer_start': 271, 'text': 'gauche'}]     1  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 85.3 ms\n"
     ]
    }
   ],
   "source": [
    "valid_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3188 768\n",
      "time: 11 ms\n"
     ]
    }
   ],
   "source": [
    "bis_valid = valid_data.drop_duplicates(subset=[\"context\"])\n",
    "print(len(valid_data), len(bis_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "time: 1min 4s\n"
     ]
    }
   ],
   "source": [
    "store_data(bis_valid, name_output = \"encoding_valid.ann\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.94 s\n"
     ]
    }
   ],
   "source": [
    "model, embeddings = import_enc(\"encoding_valid.ann\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 963 Âµs\n"
     ]
    }
   ],
   "source": [
    "query_text = \"Sur quelle jambe les personnages se tiennent-t-ils ?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 32.4 ms\n"
     ]
    }
   ],
   "source": [
    "pred_val = produce_prediction(query_text, model, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Les deux panneaux prÃ©sentent de nombreuses similitudes : chacun comporte un seul personnage exposÃ© en pied, qui se tient dans une niche en trompe-l'Åil proposant les mÃªmes dÃ©gradÃ©s de gris. Les personnages se tiennent en appui sur leur jambe droite et avancent leur pied gauche vers le spectateur. Des ailes s'ouvrent lÃ©gÃ¨rement dans leur dos, qui indiquent leur nature d'ange. Les cheveux longs et bouclÃ©s, ils sont vÃªtus d'une longue robe de couleur dont le col est rond pour l'un et carrÃ© pour l'autre. Chacun tient un instrument de musique dont il semble jouer. Leurs diffÃ©rences tiennent dans l'instrument duquel ils jouent, leur posture pour le faire ainsi que l'aspect et la position de leur tÃªte. Ainsi, l'ange en vert joue de la lira da braccio ; il semble frotter l'archet sur les cordes. Il incline la tÃªte vers son instrument selon un port identique Ã  celui de la Vierge dans La Vierge aux rochers de LÃ©onard de Vinci. L'ange en rouge joue du luth ; sa main pince les cordes de l'instrument. Son visage, vu de profil, est tournÃ© vers la gauche du spectateur.\n",
      "\n",
      "L'affiche du film a Ã©tÃ© dessinÃ©e par l'illustrateur Christophe Blain en 2010. Elle reprÃ©sente un portrait lÃ©gÃ¨rement caricatural d'un petit Joachim Zand (Mathieu Amalric) debout entre les jambes d'une vibrionnante strip-teaseuse Â« gulliverienne Â» allongÃ©e et orientÃ©e de trois-quarts, le bras en l'air et en gants bleus avec des Ã©toiles de la mÃªme couleur couvrant ses seins. Le fond de l'affiche est rouge et le titre est composÃ© de lettres en capitales jaunes. Alain Korkos pour le site @rret sur images analyse que cette affiche est un hommage Ã  celles dessinÃ©es par l'affichiste et illustrateur RenÃ© Gruau (1909-2004) en 1963 pour le Moulin Rouge prÃ©sentant une danseuse de french cancan dans une position relativement similaire avec exactement la mÃªme composition visuelle et le mÃªme choix de couleurs. Cette affiche et ses sources d'inspiration influenceront le graphisme, les couleurs, et les thÃ©matiques de celle du film La VÃ©nus Ã  la fourrure (2013) de Roman Polanski Ã©galement avec Mathieu Amalric dans le rÃ´le principal.\n",
      "\n",
      "PÃ©gase et sa lÃ©gende continuent Ã  Ãªtre Ã©normÃ©ment employÃ©s ou Ã©voquÃ©s dans la culture populaire, que ce soit par le nom et la symbolique, comme en dressage Ã©questre oÃ¹ un pÃ©gase en pesade est une figure Ã©questre oÃ¹ le cheval s'Ã©lÃ¨ve, comme s'il s'apprÃªtait Ã  s'envoler, ou comme emblÃ¨me et logo, principalement par des entreprises, l'armÃ©e, le domaine aÃ©ronautique et en numismatique. Ã l'instar d'autres crÃ©atures telles que la licorne ou le dragon, PÃ©gase a trouvÃ© sa place au cinÃ©ma, dans les littÃ©ratures de l'imaginaire et dans les bestiaires de nombreux jeux de rÃ´le et jeux vidÃ©o.\n",
      "time: 7.95 ms\n"
     ]
    }
   ],
   "source": [
    "print(*reduce(pd.DataFrame.append, map(lambda i: bis_valid[bis_valid.c_id == i], pred_val[0]))[\"context\"].tolist(), sep = '\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This prediction is accurate. Let's see what if the proportion of good prediction over all the questions of the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2 ms\n"
     ]
    }
   ],
   "source": [
    "def evaluate_recall(data):\n",
    "    \n",
    "    real_vs_pred = {\"actual_context_id\":[], \"predicted_context_id\":[]}\n",
    "\n",
    "    for i, row in data.iterrows():\n",
    "        pred = produce_prediction(row[\"question\"], model, embeddings, top_n = 1)[0][0]\n",
    "        real_vs_pred[\"actual_context_id\"].append(row[\"c_id\"])\n",
    "        real_vs_pred[\"predicted_context_id\"].append(pred)\n",
    "    predictions = pd.DataFrame(real_vs_pred, columns = real_vs_pred.keys())\n",
    "    print(\"Done\")\n",
    "    \n",
    "    li_id = predictions[\"actual_context_id\"].unique().tolist()\n",
    "\n",
    "    TP = []\n",
    "    FN = []\n",
    "    FP = []\n",
    "\n",
    "    for id_context in li_id:\n",
    "\n",
    "        pred_per_id = predictions[predictions[\"actual_context_id\"] == id_context]\n",
    "        TP.append(len([i for i in pred_per_id[\"predicted_context_id\"].tolist() if i == id_context]))\n",
    "        FN.append(len([i for i in pred_per_id[\"predicted_context_id\"].tolist() if i != id_context]))\n",
    "        \n",
    "        pred_per_id = predictions[predictions[\"predicted_context_id\"] == id_context]\n",
    "        FP.append(len([i for i in pred_per_id[\"actual_context_id\"].tolist() if i != id_context]))\n",
    "        \n",
    "    return sum(TP) / (sum(TP) + 1/2 * (sum(FP) + sum(FN)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.34598494353826853"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1min 17s\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(valid_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I got quite a low F1 score. This can be explained by the fact that the model was not trained on the training set (it would take too much take). An other explanation would be the fact that we could have the real context as the second or third prediction for a given question.\n",
    "If I chose to evaluate the model on 3 predictions per question(for example), I might have got a higher F1 score."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
