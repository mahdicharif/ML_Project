{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I chose to use transformers, through project BERT, in order to take the context of the embeddings into account.\n",
    "The embeddings of the contexts (paragraphs) will be generated once and stored in a file. This same file will be used as an input in the algorithm and read to compare the contexts with the embedding of the question.\n",
    "\n",
    "NB : I will not be using the whole training set (in order not to go beyond the capacities of my computer while generating the embeddings). Also, I did not train the BERT model on the train data (it would require too much time and the access to GPU ressources)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zipfile import ZipFile \n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from annoy import AnnoyIndex\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import os\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autotime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2 ms\n"
     ]
    }
   ],
   "source": [
    "def json_to_dataframe(input_file_path, record_path = ['data','paragraphs','qas','answers'],\n",
    "                           verbose = 1):\n",
    "    \"\"\"\n",
    "    input_file_path: path to the squad json file.\n",
    "    record_path: path to deepest level in json file default value is\n",
    "    ['data','paragraphs','qas','answers']\n",
    "    verbose: 0 to suppress it default is 1\n",
    "    \"\"\"\n",
    "    if verbose:\n",
    "        print(\"Reading the json file\")    \n",
    "    with ZipFile(input_file_path, \"r\") as z:\n",
    "        for filename in z.namelist():  \n",
    "            print(filename)  \n",
    "            with z.open(filename) as f:  \n",
    "                data = f.read()  \n",
    "                file = json.loads(data.decode(\"utf-8\"))\n",
    "    if verbose:\n",
    "        print(\"processing...\")\n",
    "    # parsing different level's in the json file\n",
    "    js = pd.io.json.json_normalize(file , record_path )\n",
    "    m = pd.io.json.json_normalize(file, record_path[:-1] )\n",
    "    r = pd.io.json.json_normalize(file,record_path[:-2])\n",
    "\n",
    "    #combining it into single dataframe\n",
    "    idx = np.repeat(r['context'].values, r.qas.str.len())\n",
    "    #     ndx  = np.repeat(m['id'].values,m['answers'].str.len())\n",
    "    m['context'] = idx\n",
    "    #     js['q_idx'] = ndx\n",
    "    main = m[['id','question','context','answers']].set_index('id').reset_index()\n",
    "    main['c_id'] = main['context'].factorize()[0]\n",
    "    if verbose:\n",
    "        print(\"shape of the dataframe is {}\".format(main.shape))\n",
    "        print(\"Done\")\n",
    "    return main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading the json file\n",
      "train.json\n",
      "processing...\n",
      "shape of the dataframe is (20731, 5)\n",
      "Done\n",
      "time: 943 ms\n"
     ]
    }
   ],
   "source": [
    "input_file_path = 'train.json.zip'\n",
    "record_path = ['data','paragraphs','qas','answers']\n",
    "verbose = 0\n",
    "train_data = json_to_dataframe(input_file_path, record_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20731"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 6 ms\n"
     ]
    }
   ],
   "source": [
    "len(train_data[\"context\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4920"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 29 ms\n"
     ]
    }
   ],
   "source": [
    "len(train_data[\"context\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4920"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 37.8 ms\n"
     ]
    }
   ],
   "source": [
    "bis = train_data.drop_duplicates(subset=[\"context\"])\n",
    "len(bis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining a function to store the embeddings in order to boost the process "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1 ms\n"
     ]
    }
   ],
   "source": [
    "def store_data(data, name_output = \"encoding.ann\"):\n",
    "\n",
    "    items = data\n",
    "    \n",
    "    model = SentenceTransformer('distiluse-base-multilingual-cased')\n",
    "\n",
    "    f = 512\n",
    "    t = AnnoyIndex(f, 'angular')  # Length of item vector that will be indexed\n",
    "    \n",
    "    items = items.set_index(\"c_id\")\n",
    "    \n",
    "    for i, row in items.iterrows():\n",
    "        v = model.encode(row[\"context\"])\n",
    "        t.add_item(i, v)\n",
    "\n",
    "    t.build(10) # 10 trees\n",
    "    t.save(name_output)\n",
    "\n",
    "    print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "time: 9.86 s\n"
     ]
    }
   ],
   "source": [
    "store_data(bis[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This operation is only done once in a while. When new contexts are added, the file must be updated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the embeddings and encoding the query to compare it to them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The metric used to compute the similarity between the query and each context is the cosine similarity (the parameter \"angular\" was used as a metric in annoy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 963 µs\n"
     ]
    }
   ],
   "source": [
    "def load_corpus(file_name = 'encoding.ann'):\n",
    "    f = 512\n",
    "    u = AnnoyIndex(f, 'angular')\n",
    "    u.load(file_name)  # super fast, will just mmap the file\n",
    "    return u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.01 ms\n"
     ]
    }
   ],
   "source": [
    "def import_enc(file_name = \"encoding.ann\"):\n",
    "    model = SentenceTransformer('distiluse-base-multilingual-cased')\n",
    "    embeddings = load_corpus(file_name)\n",
    "    return(model, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in callback <function LineWatcher.stop at 0x0000017B8B5A31F8> (for post_run_cell):\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\backcall\\backcall.py\u001b[0m in \u001b[0;36madapted\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    102\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[1;31m#            print(args, kwargs, unmatched_pos, cut_positional, unmatched_kw)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 104\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    105\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0madapted\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\autotime.py\u001b[0m in \u001b[0;36mstop\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart_time\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m             \u001b[0mdiff\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m             \u001b[1;32massert\u001b[0m \u001b[0mdiff\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'time: %s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mformat_delta\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdiff\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def produce_prediction(query_text, model, embeddings, top_n = 3):\n",
    "    query = model.encode(query_text)\n",
    "    nearest = embeddings.get_nns_by_vector(query, top_n, include_distances = True)\n",
    "    return nearest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.9 s\n"
     ]
    }
   ],
   "source": [
    "model, embeddings = import_enc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.04 ms\n"
     ]
    }
   ],
   "source": [
    "query_text = \"Pour quelle raison on ne put plus observer Cérès ?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([17, 2, 18], [1.2273740768432617, 1.2467377185821533, 1.2482205629348755])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 37.7 ms\n"
     ]
    }
   ],
   "source": [
    "nearest = produce_prediction(query_text, model, embeddings)\n",
    "nearest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lorsque Cérès est en opposition à proximité de son périhélie, il peut atteindre une magnitude apparente de +6,7. On considère généralement que cette valeur est trop faible pour que l'objet soit visible à l'œil nu, mais il est néanmoins possible pour une personne dotée d'une excellente vue et dans des conditions d'observation exceptionnelles de percevoir la planète naine. Les seuls astéroïdes pouvant atteindre une telle magnitude sont Vesta et, pendant de rares oppositions à leur périhélie, Pallas et Iris. Au maximum de sa luminosité, Cérès n'est pas l'astéroïde le plus brillant ; Vesta peut atteindre la magnitude +5,4, la dernière fois en mai et juin 2007. Aux conjonctions, Cérès atteint la magnitude de +9,3, ce qui correspond aux objets les moins lumineux qui puissent être visibles à l'aide de jumelles 10×50. La planète naine peut donc être vue aux jumelles dès qu'elle est au-dessus de l'horizon par une nuit noire. Pallas et Iris sont invisibles aux jumelles par de petites élongations.\n",
      "\n",
      "Peu après sa découverte, Cérès s'approcha trop près du Soleil et ne put être observée à nouveau ; les autres astronomes ne purent confirmer les observations de Piazzi avant la fin de l'année. Cependant, après une telle durée, il était difficile de prédire la position exacte de Cérès. Afin de retrouver l'astéroïde, Carl Friedrich Gauss développa une méthode de déduction de l'orbite basée sur trois observations. En l'espace de quelques semaines, il prédit celle de Cérès et communiqua ses résultats à Franz Xaver von Zach, éditeur du Monatliche Correspondenz. Le 31 décembre 1801, von Zach et Heinrich Olbers confirmèrent que Cérès avait été retrouvée près de la position prévue, validant ainsi la méthode.\n",
      "\n",
      "En mars 2018, de nouvelles conclusions sont publiées sur la base de données observées par Dawn d'avril à octobre 2016. Elles mettent en évidence une structure plus complexe que prévu, et surtout elle montre qu'il s'agit d'une planète naine encore très active avec une augmentation de la quantité d'eau glacée sur les murs de cratère,. Il s'agit là de la première fois où une évolution de la surface de Cérès est mise en évidence. Ces observations permettent également au grand public de découvrir des images extraordinaires de Cérès.\n",
      "time: 7 ms\n"
     ]
    }
   ],
   "source": [
    "print(*reduce(pd.DataFrame.append, map(lambda i: bis[bis.c_id == i], nearest[0]))[\"context\"].tolist(), sep = '\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I can see that the contexts are \"close\" to the query (in terms of main subject). The problem here is that, just by comparing the closest context to the real context linked to that query, I can see that I did not get the right context. The similiarities between the right one (1.2467377185821533) and the one predicted (1.2273740768432617) are quite close.\n",
    "I can also underline that the predicted context contains the word \"observation\", \"invisibles\", \"percevoir\", thus leading the algorithm to pick him as the closest context (the query contains the word \"observer\")."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate the model, I will use the F1 score. The score would surely be greater with a training of the BERT model on our data.\n",
    "For each context, I am computing the terms required for the calculation of the F1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading the json file\n",
      "valid.json\n",
      "processing...\n",
      "shape of the dataframe is (3188, 5)\n",
      "Done\n",
      "time: 141 ms\n"
     ]
    }
   ],
   "source": [
    "input_file_path_valid = 'valid.json.zip'\n",
    "record_path_valid = ['data','paragraphs','qas','answers']\n",
    "verbose = 0\n",
    "valid_data = json_to_dataframe(input_file_path_valid, record_path_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>question</th>\n",
       "      <th>context</th>\n",
       "      <th>answers</th>\n",
       "      <th>c_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>a524504f-0816-4f58-9f2d-27f82a85c73d</td>\n",
       "      <td>Que concerne principalement les documents ?</td>\n",
       "      <td>Les deux tableaux sont certes décrits par des ...</td>\n",
       "      <td>[{'answer_start': 161, 'text': 'La Vierge aux ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>8a72ad1c-b2fe-4fe6-9f87-35fcb713cf38</td>\n",
       "      <td>Par quoi sont décrit les deux tableaux ?</td>\n",
       "      <td>Les deux tableaux sont certes décrits par des ...</td>\n",
       "      <td>[{'answer_start': 46, 'text': 'documents conte...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>b2db7f77-f6a7-4c3d-9274-9807f9764e97</td>\n",
       "      <td>Quels types d'objets sont les deux tableaux au...</td>\n",
       "      <td>Les deux tableaux sont certes décrits par des ...</td>\n",
       "      <td>[{'answer_start': 204, 'text': 'objets de spéc...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>31ec5079-7d62-43b9-8a5c-49f8b7ad6f5a</td>\n",
       "      <td>Sur quelle jambe les personnages se tiennent-t...</td>\n",
       "      <td>Les deux panneaux présentent de nombreuses sim...</td>\n",
       "      <td>[{'answer_start': 242, 'text': 'droite'}]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>008a2ca6-a589-4751-ba59-f7f762874785</td>\n",
       "      <td>Quel pied avancent les personnages ?</td>\n",
       "      <td>Les deux panneaux présentent de nombreuses sim...</td>\n",
       "      <td>[{'answer_start': 271, 'text': 'gauche'}]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     id  \\\n",
       "0  a524504f-0816-4f58-9f2d-27f82a85c73d   \n",
       "1  8a72ad1c-b2fe-4fe6-9f87-35fcb713cf38   \n",
       "2  b2db7f77-f6a7-4c3d-9274-9807f9764e97   \n",
       "3  31ec5079-7d62-43b9-8a5c-49f8b7ad6f5a   \n",
       "4  008a2ca6-a589-4751-ba59-f7f762874785   \n",
       "\n",
       "                                            question  \\\n",
       "0        Que concerne principalement les documents ?   \n",
       "1           Par quoi sont décrit les deux tableaux ?   \n",
       "2  Quels types d'objets sont les deux tableaux au...   \n",
       "3  Sur quelle jambe les personnages se tiennent-t...   \n",
       "4              Quel pied avancent les personnages ?    \n",
       "\n",
       "                                             context  \\\n",
       "0  Les deux tableaux sont certes décrits par des ...   \n",
       "1  Les deux tableaux sont certes décrits par des ...   \n",
       "2  Les deux tableaux sont certes décrits par des ...   \n",
       "3  Les deux panneaux présentent de nombreuses sim...   \n",
       "4  Les deux panneaux présentent de nombreuses sim...   \n",
       "\n",
       "                                             answers  c_id  \n",
       "0  [{'answer_start': 161, 'text': 'La Vierge aux ...     0  \n",
       "1  [{'answer_start': 46, 'text': 'documents conte...     0  \n",
       "2  [{'answer_start': 204, 'text': 'objets de spéc...     0  \n",
       "3          [{'answer_start': 242, 'text': 'droite'}]     1  \n",
       "4          [{'answer_start': 271, 'text': 'gauche'}]     1  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 85.3 ms\n"
     ]
    }
   ],
   "source": [
    "valid_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3188 768\n",
      "time: 11 ms\n"
     ]
    }
   ],
   "source": [
    "bis_valid = valid_data.drop_duplicates(subset=[\"context\"])\n",
    "print(len(valid_data), len(bis_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "time: 1min 4s\n"
     ]
    }
   ],
   "source": [
    "store_data(bis_valid, name_output = \"encoding_valid.ann\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.94 s\n"
     ]
    }
   ],
   "source": [
    "model, embeddings = import_enc(\"encoding_valid.ann\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 963 µs\n"
     ]
    }
   ],
   "source": [
    "query_text = \"Sur quelle jambe les personnages se tiennent-t-ils ?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 32.4 ms\n"
     ]
    }
   ],
   "source": [
    "pred_val = produce_prediction(query_text, model, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Les deux panneaux présentent de nombreuses similitudes : chacun comporte un seul personnage exposé en pied, qui se tient dans une niche en trompe-l'œil proposant les mêmes dégradés de gris. Les personnages se tiennent en appui sur leur jambe droite et avancent leur pied gauche vers le spectateur. Des ailes s'ouvrent légèrement dans leur dos, qui indiquent leur nature d'ange. Les cheveux longs et bouclés, ils sont vêtus d'une longue robe de couleur dont le col est rond pour l'un et carré pour l'autre. Chacun tient un instrument de musique dont il semble jouer. Leurs différences tiennent dans l'instrument duquel ils jouent, leur posture pour le faire ainsi que l'aspect et la position de leur tête. Ainsi, l'ange en vert joue de la lira da braccio ; il semble frotter l'archet sur les cordes. Il incline la tête vers son instrument selon un port identique à celui de la Vierge dans La Vierge aux rochers de Léonard de Vinci. L'ange en rouge joue du luth ; sa main pince les cordes de l'instrument. Son visage, vu de profil, est tourné vers la gauche du spectateur.\n",
      "\n",
      "L'affiche du film a été dessinée par l'illustrateur Christophe Blain en 2010. Elle représente un portrait légèrement caricatural d'un petit Joachim Zand (Mathieu Amalric) debout entre les jambes d'une vibrionnante strip-teaseuse « gulliverienne » allongée et orientée de trois-quarts, le bras en l'air et en gants bleus avec des étoiles de la même couleur couvrant ses seins. Le fond de l'affiche est rouge et le titre est composé de lettres en capitales jaunes. Alain Korkos pour le site @rret sur images analyse que cette affiche est un hommage à celles dessinées par l'affichiste et illustrateur René Gruau (1909-2004) en 1963 pour le Moulin Rouge présentant une danseuse de french cancan dans une position relativement similaire avec exactement la même composition visuelle et le même choix de couleurs. Cette affiche et ses sources d'inspiration influenceront le graphisme, les couleurs, et les thématiques de celle du film La Vénus à la fourrure (2013) de Roman Polanski également avec Mathieu Amalric dans le rôle principal.\n",
      "\n",
      "Pégase et sa légende continuent à être énormément employés ou évoqués dans la culture populaire, que ce soit par le nom et la symbolique, comme en dressage équestre où un pégase en pesade est une figure équestre où le cheval s'élève, comme s'il s'apprêtait à s'envoler, ou comme emblème et logo, principalement par des entreprises, l'armée, le domaine aéronautique et en numismatique. À l'instar d'autres créatures telles que la licorne ou le dragon, Pégase a trouvé sa place au cinéma, dans les littératures de l'imaginaire et dans les bestiaires de nombreux jeux de rôle et jeux vidéo.\n",
      "time: 7.95 ms\n"
     ]
    }
   ],
   "source": [
    "print(*reduce(pd.DataFrame.append, map(lambda i: bis_valid[bis_valid.c_id == i], pred_val[0]))[\"context\"].tolist(), sep = '\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This prediction is accurate. Let's see what if the proportion of good prediction over all the questions of the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2 ms\n"
     ]
    }
   ],
   "source": [
    "def evaluate_recall(data):\n",
    "    \n",
    "    real_vs_pred = {\"actual_context_id\":[], \"predicted_context_id\":[]}\n",
    "\n",
    "    for i, row in data.iterrows():\n",
    "        pred = produce_prediction(row[\"question\"], model, embeddings, top_n = 1)[0][0]\n",
    "        real_vs_pred[\"actual_context_id\"].append(row[\"c_id\"])\n",
    "        real_vs_pred[\"predicted_context_id\"].append(pred)\n",
    "    predictions = pd.DataFrame(real_vs_pred, columns = real_vs_pred.keys())\n",
    "    print(\"Done\")\n",
    "    \n",
    "    li_id = predictions[\"actual_context_id\"].unique().tolist()\n",
    "\n",
    "    TP = []\n",
    "    FN = []\n",
    "    FP = []\n",
    "\n",
    "    for id_context in li_id:\n",
    "\n",
    "        pred_per_id = predictions[predictions[\"actual_context_id\"] == id_context]\n",
    "        TP.append(len([i for i in pred_per_id[\"predicted_context_id\"].tolist() if i == id_context]))\n",
    "        FN.append(len([i for i in pred_per_id[\"predicted_context_id\"].tolist() if i != id_context]))\n",
    "        \n",
    "        pred_per_id = predictions[predictions[\"predicted_context_id\"] == id_context]\n",
    "        FP.append(len([i for i in pred_per_id[\"actual_context_id\"].tolist() if i != id_context]))\n",
    "        \n",
    "    return sum(TP) / (sum(TP) + 1/2 * (sum(FP) + sum(FN)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.34598494353826853"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1min 17s\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(valid_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I got quite a low F1 score. This can be explained by the fact that the model was not trained on the training set (it would take too much take). An other explanation would be the fact that we could have the real context as the second or third prediction for a given question.\n",
    "If I chose to evaluate the model on 3 predictions per question(for example), I might have got a higher F1 score."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
